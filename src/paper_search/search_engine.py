#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡æœç´¢å¼•æ“æ¨¡å—
æ”¯æŒarXivå’ŒSemantic Scholar API
"""

import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional
from datetime import datetime
import re
from dataclasses import dataclass

from ..llm.llm_client import LLMClient


@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    abstract: str
    published_date: str
    pdf_url: str
    arxiv_id: Optional[str] = None
    doi: Optional[str] = None
    categories: List[str] = None
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = []


class PaperSearchEngine:
    """è®ºæ–‡æœç´¢å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm_client = LLMClient(config)
        self.arxiv_config = config["paper_search"]["arxiv"]
        self.semantic_scholar_config = config["paper_search"]["semantic_scholar"]
    
    def search(self, query: str, source: str = "arxiv", max_results: int = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            source: æœç´¢æº (arxiv, semantic_scholar)
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        # ä½¿ç”¨LLMä¼˜åŒ–æŸ¥è¯¢
        optimized_query = self._optimize_query(query)
        
        if source == "arxiv":
            return self._search_arxiv(optimized_query, max_results)
        elif source == "semantic_scholar":
            return self._search_semantic_scholar(optimized_query, max_results)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢æº: {source}")
    
    def _optimize_query(self, query: str) -> str:
        """ä½¿ç”¨LLMä¼˜åŒ–æœç´¢æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æœç´¢ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºé€‚åˆå­¦æœ¯è®ºæ–‡æœç´¢çš„å…³é”®è¯ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·æå–æœ€é‡è¦çš„å­¦æœ¯å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç¤ºä¾‹:
ç”¨æˆ·æŸ¥è¯¢: "æœ€è¿‘æœ‰å“ªäº›å…³äºDiffusionæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä¸­çš„åº”ç”¨ï¼Ÿ"
å…³é”®è¯: diffusion model medical image application

ç”¨æˆ·æŸ¥è¯¢: "å›¾ç¥ç»ç½‘ç»œåœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨"
å…³é”®è¯: graph neural network drug discovery

ç°åœ¨è¯·å¤„ç†ç”¨æˆ·æŸ¥è¯¢:
"""
        
        try:
            response = self.llm_client.generate(prompt)
            # æ¸…ç†å“åº”ï¼Œåªä¿ç•™å…³é”®è¯
            optimized = re.sub(r'[^a-zA-Z0-9\s\u4e00-\u9fff]', ' ', response.strip())
            optimized = ' '.join(optimized.split())  # è§„èŒƒåŒ–ç©ºæ ¼
            return optimized if optimized else query
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")
            return query
    
    def _search_arxiv(self, query: str, max_results: int = None) -> List[Paper]:
        """æœç´¢arXiv
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if max_results is None:
            max_results = self.arxiv_config["max_results"]
        
        # æ„å»ºarXiv APIæŸ¥è¯¢
        params = {
            "search_query": f"all:{query}",
            "start": 0,
            "max_results": max_results,
            "sortBy": self.arxiv_config["sort_by"],
            "sortOrder": self.arxiv_config["sort_order"]
        }
        
        try:
            response = requests.get(self.arxiv_config["base_url"], params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_arxiv_response(response.text)
        except Exception as e:
            print(f"âŒ arXivæœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_content: str) -> List[Paper]:
        """è§£æarXiv APIå“åº”
        
        Args:
            xml_content: XMLå“åº”å†…å®¹
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                # æå–åŸºæœ¬ä¿¡æ¯
                title = entry.find('atom:title', namespaces).text.strip().replace('\n', ' ')
                abstract = entry.find('atom:summary', namespaces).text.strip().replace('\n', ' ')
                
                # æå–ä½œè€…
                authors = []
                for author in entry.findall('atom:author', namespaces):
                    name = author.find('atom:name', namespaces).text
                    authors.append(name)
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published = entry.find('atom:published', namespaces).text
                published_date = published.split('T')[0]  # åªä¿ç•™æ—¥æœŸéƒ¨åˆ†
                
                # æå–PDFé“¾æ¥å’ŒarXiv ID
                pdf_url = None
                arxiv_id = None
                
                for link in entry.findall('atom:link', namespaces):
                    if link.get('type') == 'application/pdf':
                        pdf_url = link.get('href')
                
                # ä»IDä¸­æå–arXiv ID
                entry_id = entry.find('atom:id', namespaces).text
                arxiv_id = entry_id.split('/')[-1]
                
                # æå–åˆ†ç±»
                categories = []
                for category in entry.findall('atom:category', namespaces):
                    categories.append(category.get('term'))
                
                paper = Paper(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    published_date=published_date,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    categories=categories
                )
                
                papers.append(paper)
        
        except Exception as e:
            print(f"âŒ è§£æarXivå“åº”å¤±è´¥: {e}")
        
        return papers
    
    def _search_semantic_scholar(self, query: str, max_results: int = None) -> List[Paper]:
        """æœç´¢Semantic Scholar
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if max_results is None:
            max_results = self.semantic_scholar_config["max_results"]
        
        headers = {}
        if self.semantic_scholar_config.get("api_key"):
            headers["x-api-key"] = self.semantic_scholar_config["api_key"]
        
        params = {
            "query": query,
            "limit": max_results,
            "fields": "title,authors,abstract,year,openAccessPdf,externalIds"
        }
        
        try:
            url = f"{self.semantic_scholar_config['base_url']}/paper/search"
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            return self._parse_semantic_scholar_response(response.json())
        except Exception as e:
            print(f"âŒ Semantic Scholaræœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_semantic_scholar_response(self, json_data: Dict[str, Any]) -> List[Paper]:
        """è§£æSemantic Scholar APIå“åº”
        
        Args:
            json_data: JSONå“åº”æ•°æ®
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            for item in json_data.get('data', []):
                # æå–åŸºæœ¬ä¿¡æ¯
                title = item.get('title', '').strip()
                abstract = item.get('abstract', '').strip() if item.get('abstract') else ''
                
                # æå–ä½œè€…
                authors = []
                for author in item.get('authors', []):
                    authors.append(author.get('name', ''))
                
                # æå–å¹´ä»½
                year = item.get('year')
                published_date = str(year) if year else ''
                
                # æå–PDFé“¾æ¥
                pdf_url = None
                open_access_pdf = item.get('openAccessPdf')
                if open_access_pdf:
                    pdf_url = open_access_pdf.get('url')
                
                # æå–DOIå’ŒarXiv ID
                doi = None
                arxiv_id = None
                external_ids = item.get('externalIds', {})
                if external_ids:
                    doi = external_ids.get('DOI')
                    arxiv_id = external_ids.get('ArXiv')
                
                paper = Paper(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    published_date=published_date,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    doi=doi
                )
                
                papers.append(paper)
        
        except Exception as e:
            print(f"âŒ è§£æSemantic Scholarå“åº”å¤±è´¥: {e}")
        
        return papers
    
    def display_results(self, papers: List[Paper]) -> None:
        """æ˜¾ç¤ºæœç´¢ç»“æœ
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
        
        print(f"\nğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡:\n")
        
        for i, paper in enumerate(papers, 1):
            print(f"ğŸ”¸ [{i}] {paper.title}")
            print(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper.authors[:3])}{'...' if len(paper.authors) > 3 else ''}")
            print(f"   ğŸ“… å‘è¡¨: {paper.published_date}")
            
            if paper.abstract:
                # æˆªæ–­æ‘˜è¦
                abstract_preview = paper.abstract[:200] + "..." if len(paper.abstract) > 200 else paper.abstract
                print(f"   ğŸ“ æ‘˜è¦: {abstract_preview}")
            
            if paper.pdf_url:
                print(f"   ğŸ”— PDF: {paper.pdf_url}")
            
            if paper.arxiv_id:
                print(f"   ğŸ†” arXiv: {paper.arxiv_id}")
            
            if paper.categories:
                print(f"   ğŸ·ï¸  åˆ†ç±»: {', '.join(paper.categories[:3])}")
            
            print()