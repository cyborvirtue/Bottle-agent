#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡æœç´¢å¼•æ“æ¨¡å—
æ”¯æŒarXivå’ŒSemantic Scholar API
"""

import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import re
from dataclasses import dataclass

from ..llm.llm_client import LLMClient
from .tag_manager import TagManager


@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: List[str]
    abstract: str
    published_date: str
    pdf_url: str
    arxiv_id: Optional[str] = None
    doi: Optional[str] = None
    categories: List[str] = None
    
    def __post_init__(self):
        if self.categories is None:
            self.categories = []


class PaperSearchEngine:
    """è®ºæ–‡æœç´¢å¼•æ“"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm_client = LLMClient(config)
        self.arxiv_config = config["paper_search"]["arxiv"]
        self.semantic_scholar_config = config["paper_search"]["semantic_scholar"]
        self.tag_manager = TagManager(config.get("storage", {}).get("data_dir", "data"))
    
    def search(self, query: str, source: str = "arxiv", max_results: int = None, 
               start_date: str = None, end_date: str = None) -> List[Paper]:
        """æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            source: æœç´¢æº (arxiv, semantic_scholar)
            max_results: æœ€å¤§ç»“æœæ•°
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        # ä½¿ç”¨LLMä¼˜åŒ–æŸ¥è¯¢
        optimized_query = self._optimize_query(query)
        
        if source == "arxiv":
            return self._search_arxiv(optimized_query, max_results, start_date, end_date)
        elif source == "semantic_scholar":
            return self._search_semantic_scholar(optimized_query, max_results, start_date, end_date)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢æº: {source}")
    
    def search_by_time_range(self, query: str, days_back: int = 7, source: str = "arxiv", 
                            max_results: int = None) -> List[Paper]:
        """æŒ‰æ—¶é—´èŒƒå›´æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            days_back: å‘å‰æœç´¢çš„å¤©æ•°
            source: æœç´¢æº
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
        
        return self.search(query, source, max_results, start_date, end_date)
    
    def search_latest_papers(self, source: str = "arxiv", max_results: int = 50) -> List[Paper]:
        """æœç´¢æœ€æ–°è®ºæ–‡ï¼ˆç”¨äºæ ‡ç­¾æ¨é€ï¼‰
        
        Args:
            source: æœç´¢æº
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        # æœç´¢æœ€è¿‘3å¤©çš„è®ºæ–‡
        return self.search_by_time_range("", days_back=3, source=source, max_results=max_results)
    
    def check_and_notify_new_papers(self) -> int:
        """æ£€æŸ¥å¹¶æ¨é€æ–°è®ºæ–‡
        
        Returns:
            æ¨é€çš„è®ºæ–‡æ•°é‡
        """
        # è·å–ç”¨æˆ·æ ‡ç­¾
        tags = self.tag_manager.get_tags(active_only=True)
        if not tags:
            return 0
        
        # æœç´¢æœ€æ–°è®ºæ–‡
        latest_papers = self.search_latest_papers()
        notification_count = 0
        
        for paper in latest_papers:
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ¹é…ç”¨æˆ·æ ‡ç­¾
            matched_tags = self.tag_manager.match_paper_tags(
                paper.title, 
                paper.abstract, 
                paper.categories or []
            )
            
            if matched_tags:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ¨é€è¿‡
                existing_notifications = self.tag_manager.get_notifications()
                if not any(n.paper_id == paper.arxiv_id for n in existing_notifications):
                    # æ·»åŠ æ¨é€é€šçŸ¥
                    self.tag_manager.add_notification(
                        paper_id=paper.arxiv_id or paper.title[:50],
                        title=paper.title,
                        authors=paper.authors,
                        abstract=paper.abstract,
                        published_date=paper.published_date,
                        pdf_url=paper.pdf_url or "",
                        matched_tags=matched_tags
                    )
                    notification_count += 1
        
        return notification_count
    
    def _optimize_query(self, query: str) -> str:
        """ä½¿ç”¨LLMä¼˜åŒ–æœç´¢æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æœç´¢ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºé€‚åˆå­¦æœ¯è®ºæ–‡æœç´¢çš„å…³é”®è¯ã€‚

ç”¨æˆ·æŸ¥è¯¢: {query}

è¯·æå–æœ€é‡è¦çš„å­¦æœ¯å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚

ç¤ºä¾‹:
ç”¨æˆ·æŸ¥è¯¢: "æœ€è¿‘æœ‰å“ªäº›å…³äºDiffusionæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä¸­çš„åº”ç”¨ï¼Ÿ"
å…³é”®è¯: diffusion model medical image application

ç”¨æˆ·æŸ¥è¯¢: "å›¾ç¥ç»ç½‘ç»œåœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨"
å…³é”®è¯: graph neural network drug discovery

ç°åœ¨è¯·å¤„ç†ç”¨æˆ·æŸ¥è¯¢:
"""
        
        try:
            response = self.llm_client.generate(prompt)
            # æ¸…ç†å“åº”ï¼Œåªä¿ç•™å…³é”®è¯
            optimized = re.sub(r'[^a-zA-Z0-9\s\u4e00-\u9fff]', ' ', response.strip())
            optimized = ' '.join(optimized.split())  # è§„èŒƒåŒ–ç©ºæ ¼
            return optimized if optimized else query
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {e}")
            return query
    
    def _search_arxiv(self, query: str, max_results: int = None, 
                     start_date: str = None, end_date: str = None) -> List[Paper]:
        """æœç´¢arXiv
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if max_results is None:
            max_results = self.arxiv_config["max_results"]
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = f"all:{query}" if query.strip() else "cat:*"
        
        # æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤
        if start_date or end_date:
            date_filter = []
            if start_date:
                # arXivä½¿ç”¨YYYYMMDDæ ¼å¼
                start_arxiv = start_date.replace("-", "")
                date_filter.append(f"submittedDate:[{start_arxiv}0000")
            if end_date:
                end_arxiv = end_date.replace("-", "")
                if start_date:
                    date_filter.append(f"TO {end_arxiv}2359]")
                else:
                    date_filter.append(f"submittedDate:[* TO {end_arxiv}2359]")
            
            if date_filter:
                if query.strip():
                    search_query += f" AND {''.join(date_filter)}"
                else:
                    search_query = ''.join(date_filter)
        
        # æ„å»ºarXiv APIæŸ¥è¯¢
        params = {
            "search_query": search_query,
            "start": 0,
            "max_results": max_results,
            "sortBy": self.arxiv_config["sort_by"],
            "sortOrder": self.arxiv_config["sort_order"]
        }
        
        try:
            response = requests.get(self.arxiv_config["base_url"], params=params, timeout=30)
            response.raise_for_status()
            
            return self._parse_arxiv_response(response.text)
        except Exception as e:
            print(f"âŒ arXivæœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_content: str) -> List[Paper]:
        """è§£æarXiv APIå“åº”
        
        Args:
            xml_content: XMLå“åº”å†…å®¹
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                # æå–åŸºæœ¬ä¿¡æ¯
                title = entry.find('atom:title', namespaces).text.strip().replace('\n', ' ')
                abstract = entry.find('atom:summary', namespaces).text.strip().replace('\n', ' ')
                
                # æå–ä½œè€…
                authors = []
                for author in entry.findall('atom:author', namespaces):
                    name = author.find('atom:name', namespaces).text
                    authors.append(name)
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published = entry.find('atom:published', namespaces).text
                published_date = published.split('T')[0]  # åªä¿ç•™æ—¥æœŸéƒ¨åˆ†
                
                # æå–PDFé“¾æ¥å’ŒarXiv ID
                pdf_url = None
                arxiv_id = None
                
                for link in entry.findall('atom:link', namespaces):
                    if link.get('type') == 'application/pdf':
                        pdf_url = link.get('href')
                
                # ä»IDä¸­æå–arXiv ID
                entry_id = entry.find('atom:id', namespaces).text
                arxiv_id = entry_id.split('/')[-1]
                
                # æå–åˆ†ç±»
                categories = []
                for category in entry.findall('atom:category', namespaces):
                    categories.append(category.get('term'))
                
                paper = Paper(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    published_date=published_date,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    categories=categories
                )
                
                papers.append(paper)
        
        except Exception as e:
            print(f"âŒ è§£æarXivå“åº”å¤±è´¥: {e}")
        
        return papers
    
    def _search_semantic_scholar(self, query: str, max_results: int = None, 
                                start_date: str = None, end_date: str = None) -> List[Paper]:
        """æœç´¢Semantic Scholar
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        if max_results is None:
            max_results = self.semantic_scholar_config["max_results"]
        
        headers = {}
        if self.semantic_scholar_config.get("api_key"):
            headers["x-api-key"] = self.semantic_scholar_config["api_key"]
        
        params = {
            "query": query,
            "limit": max_results,
            "fields": "title,authors,abstract,year,openAccessPdf,externalIds,publicationDate"
        }
        
        # æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤
        if start_date:
            start_year = int(start_date[:4])
            params["year"] = f"{start_year}-"
        if end_date:
            end_year = int(end_date[:4])
            if "year" in params:
                params["year"] = f"{start_year}-{end_year}"
            else:
                params["year"] = f"-{end_year}"
        
        try:
            url = f"{self.semantic_scholar_config['base_url']}/paper/search"
            response = requests.get(url, params=params, headers=headers, timeout=30)
            response.raise_for_status()
            
            return self._parse_semantic_scholar_response(response.json())
        except Exception as e:
            print(f"âŒ Semantic Scholaræœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_semantic_scholar_response(self, json_data: Dict[str, Any]) -> List[Paper]:
        """è§£æSemantic Scholar APIå“åº”
        
        Args:
            json_data: JSONå“åº”æ•°æ®
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        
        try:
            for item in json_data.get('data', []):
                # æå–åŸºæœ¬ä¿¡æ¯
                title = item.get('title', '').strip()
                abstract = item.get('abstract', '').strip() if item.get('abstract') else ''
                
                # æå–ä½œè€…
                authors = []
                for author in item.get('authors', []):
                    authors.append(author.get('name', ''))
                
                # æå–å¹´ä»½
                year = item.get('year')
                published_date = str(year) if year else ''
                
                # æå–PDFé“¾æ¥
                pdf_url = None
                open_access_pdf = item.get('openAccessPdf')
                if open_access_pdf:
                    pdf_url = open_access_pdf.get('url')
                
                # æå–DOIå’ŒarXiv ID
                doi = None
                arxiv_id = None
                external_ids = item.get('externalIds', {})
                if external_ids:
                    doi = external_ids.get('DOI')
                    arxiv_id = external_ids.get('ArXiv')
                
                paper = Paper(
                    title=title,
                    authors=authors,
                    abstract=abstract,
                    published_date=published_date,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    doi=doi
                )
                
                papers.append(paper)
        
        except Exception as e:
            print(f"âŒ è§£æSemantic Scholarå“åº”å¤±è´¥: {e}")
        
        return papers
    
    def display_results(self, papers: List[Paper]) -> None:
        """æ˜¾ç¤ºæœç´¢ç»“æœ
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡")
            return
        
        print(f"\nğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡:\n")
        
        for i, paper in enumerate(papers, 1):
            print(f"ğŸ”¸ [{i}] {paper.title}")
            print(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper.authors[:3])}{'...' if len(paper.authors) > 3 else ''}")
            print(f"   ğŸ“… å‘è¡¨: {paper.published_date}")
            
            if paper.abstract:
                # æˆªæ–­æ‘˜è¦
                abstract_preview = paper.abstract[:200] + "..." if len(paper.abstract) > 200 else paper.abstract
                print(f"   ğŸ“ æ‘˜è¦: {abstract_preview}")
            
            if paper.pdf_url:
                print(f"   ğŸ”— PDF: {paper.pdf_url}")
            
            if paper.arxiv_id:
                print(f"   ğŸ†” arXiv: {paper.arxiv_id}")
            
            if paper.categories:
                print(f"   ğŸ·ï¸  åˆ†ç±»: {', '.join(paper.categories[:3])}")
            
            print()