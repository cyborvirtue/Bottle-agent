#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGçŸ¥è¯†åº“ç®¡ç†æ¨¡å—
æ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€æ–‡æ¡£å¤„ç†ã€å‘é‡æ£€ç´¢
"""

import os
import json
import pickle
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import logging
from datetime import datetime

import faiss
import numpy as np
from dataclasses import dataclass, asdict

from .document_processor import DocumentProcessor
from .embedding_client import EmbeddingClient
from ..llm.llm_client import LLMClient


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""
    id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None


@dataclass
class KnowledgeBaseInfo:
    """çŸ¥è¯†åº“ä¿¡æ¯"""
    name: str
    description: str
    folder_path: str
    created_at: str
    updated_at: str
    document_count: int
    chunk_count: int


class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.rag_config = config["rag"]
        self.kb_config = config["knowledge_base"]
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.document_processor = DocumentProcessor(config)
        self.embedding_client = EmbeddingClient(config)
        self.llm_client = LLMClient(config)
        
        # å­˜å‚¨è·¯å¾„
        self.storage_path = Path(self.kb_config["storage_path"])
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # çŸ¥è¯†åº“ç´¢å¼•æ–‡ä»¶
        self.index_file = self.storage_path / "knowledge_bases.json"
        
        # åŠ è½½çŸ¥è¯†åº“ç´¢å¼•
        self.knowledge_bases = self._load_knowledge_bases_index()
    
    def _load_knowledge_bases_index(self) -> Dict[str, KnowledgeBaseInfo]:
        """åŠ è½½çŸ¥è¯†åº“ç´¢å¼•
        
        Returns:
            çŸ¥è¯†åº“ç´¢å¼•å­—å…¸
        """
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸ºKnowledgeBaseInfoå¯¹è±¡
                knowledge_bases = {}
                for name, info in data.items():
                    knowledge_bases[name] = KnowledgeBaseInfo(**info)
                
                return knowledge_bases
            except Exception as e:
                logging.error(f"åŠ è½½çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
        
        return {}
    
    def _save_knowledge_bases_index(self):
        """ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•"""
        try:
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            data = {}
            for name, info in self.knowledge_bases.items():
                data[name] = asdict(info)
            
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
    
    def create_knowledge_base(self, name: str, folder_path: str, description: str = "") -> bool:
        """åˆ›å»ºçŸ¥è¯†åº“
        
        Args:
            name: çŸ¥è¯†åº“åç§°
            folder_path: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
            description: çŸ¥è¯†åº“æè¿°
            
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        if name in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{name}' å·²å­˜åœ¨")
            return False
        
        folder_path = Path(folder_path)
        if not folder_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
            return False
        
        print(f"ğŸ“š åˆ›å»ºçŸ¥è¯†åº“: {name}")
        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {folder_path}")
        
        try:
            # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
            kb_path = self.storage_path / name
            kb_path.mkdir(exist_ok=True)
            
            # å¤„ç†æ–‡æ¡£
            print("ğŸ“„ å¤„ç†æ–‡æ¡£...")
            documents = self.document_processor.process_folder(str(folder_path))
            
            if not documents:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
                return False
            
            # åˆ†å—
            print("âœ‚ï¸  æ–‡æ¡£åˆ†å—...")
            chunks = self.document_processor.chunk_documents(documents)
            
            # ç”ŸæˆåµŒå…¥
            print("ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡...")
            embeddings = self.embedding_client.embed_texts([chunk.content for chunk in chunks])
            
            # æ·»åŠ åµŒå…¥åˆ°å—ä¸­
            for chunk, embedding in zip(chunks, embeddings):
                chunk.embedding = embedding
            
            # æ„å»ºå‘é‡ç´¢å¼•
            print("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
            self._build_vector_index(name, chunks)
            
            # ä¿å­˜å—æ•°æ®
            self._save_chunks(name, chunks)
            
            # æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯
            kb_info = KnowledgeBaseInfo(
                name=name,
                description=description,
                folder_path=str(folder_path),
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                document_count=len(documents),
                chunk_count=len(chunks)
            )
            
            self.knowledge_bases[name] = kb_info
            self._save_knowledge_bases_index()
            
            print(f"âœ… çŸ¥è¯†åº“ '{name}' åˆ›å»ºæˆåŠŸ")
            print(f"   ğŸ“Š æ–‡æ¡£æ•°: {len(documents)}, å—æ•°: {len(chunks)}")
            
            return True
        
        except Exception as e:
            logging.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            print(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def _build_vector_index(self, kb_name: str, chunks: List[DocumentChunk]):
        """æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        if not chunks:
            return
        
        # è·å–åµŒå…¥ç»´åº¦
        embedding_dim = len(chunks[0].embedding)
        
        # åˆ›å»ºFAISSç´¢å¼•
        index = faiss.IndexFlatIP(embedding_dim)  # å†…ç§¯ç›¸ä¼¼åº¦
        
        # æ·»åŠ å‘é‡
        embeddings = np.array([chunk.embedding for chunk in chunks]).astype('float32')
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)
        
        index.add(embeddings)
        
        # ä¿å­˜ç´¢å¼•
        index_path = self.storage_path / kb_name / "vector_index.faiss"
        faiss.write_index(index, str(index_path))
    
    def _save_chunks(self, kb_name: str, chunks: List[DocumentChunk]):
        """ä¿å­˜æ–‡æ¡£å—
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks_path = self.storage_path / kb_name / "chunks.pkl"
        
        # ä¿å­˜æ—¶ä¸åŒ…å«åµŒå…¥å‘é‡ï¼ˆå¤ªå¤§ï¼‰
        chunks_to_save = []
        for chunk in chunks:
            chunk_copy = DocumentChunk(
                id=chunk.id,
                content=chunk.content,
                metadata=chunk.metadata
            )
            chunks_to_save.append(chunk_copy)
        
        with open(chunks_path, 'wb') as f:
            pickle.dump(chunks_to_save, f)
    
    def _load_chunks(self, kb_name: str) -> List[DocumentChunk]:
        """åŠ è½½æ–‡æ¡£å—
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks_path = self.storage_path / kb_name / "chunks.pkl"
        
        if not chunks_path.exists():
            return []
        
        with open(chunks_path, 'rb') as f:
            return pickle.load(f)
    
    def _load_vector_index(self, kb_name: str):
        """åŠ è½½å‘é‡ç´¢å¼•
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            FAISSç´¢å¼•
        """
        index_path = self.storage_path / kb_name / "vector_index.faiss"
        
        if not index_path.exists():
            return None
        
        return faiss.read_index(str(index_path))
    
    def query(self, kb_name: str, query: str, top_k: int = None) -> str:
        """æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›çš„ç›¸å…³å—æ•°é‡
            
        Returns:
            å›ç­”
        """
        if kb_name not in self.knowledge_bases:
            return f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨"
        
        if top_k is None:
            top_k = self.rag_config["top_k"]
        
        try:
            # åŠ è½½ç´¢å¼•å’Œå—
            index = self._load_vector_index(kb_name)
            chunks = self._load_chunks(kb_name)
            
            if index is None or not chunks:
                return f"âŒ çŸ¥è¯†åº“ '{kb_name}' æ•°æ®ä¸å®Œæ•´"
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_client.embed_text(query)
            query_vector = np.array([query_embedding]).astype('float32')
            faiss.normalize_L2(query_vector)
            
            # æœç´¢ç›¸ä¼¼å—
            scores, indices = index.search(query_vector, top_k)
            
            # è·å–ç›¸å…³å—
            relevant_chunks = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if score >= self.rag_config["similarity_threshold"]:
                    chunk = chunks[idx]
                    relevant_chunks.append((chunk, score))
            
            if not relevant_chunks:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk.content}\næ¥æº: {chunk.metadata.get('source', 'æœªçŸ¥')}"
                for i, (chunk, _) in enumerate(relevant_chunks)
            ])
            
            # ç”Ÿæˆå›ç­”
            prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€è¯¦ç»†ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µç¼–å·ã€‚

å›ç­”:
"""
            
            answer = self.llm_client.generate(prompt)
            
            # æ·»åŠ å¼•ç”¨ä¿¡æ¯
            references = "\n\nğŸ“š å‚è€ƒæ–‡æ¡£:\n"
            for i, (chunk, score) in enumerate(relevant_chunks):
                source = chunk.metadata.get('source', 'æœªçŸ¥')
                page = chunk.metadata.get('page', '')
                page_info = f", ç¬¬{page}é¡µ" if page else ""
                references += f"[{i+1}] {source}{page_info} (ç›¸ä¼¼åº¦: {score:.3f})\n"
            
            return answer + references
        
        except Exception as e:
            logging.error(f"æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {e}"
    
    def list_knowledge_bases(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“
        
        Returns:
            çŸ¥è¯†åº“åç§°åˆ—è¡¨
        """
        return list(self.knowledge_bases.keys())
    
    def get_knowledge_base_info(self, kb_name: str) -> Optional[KnowledgeBaseInfo]:
        """è·å–çŸ¥è¯†åº“ä¿¡æ¯
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            çŸ¥è¯†åº“ä¿¡æ¯
        """
        return self.knowledge_bases.get(kb_name)
    
    def delete_knowledge_base(self, kb_name: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if kb_name not in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
            return False
        
        try:
            # åˆ é™¤çŸ¥è¯†åº“æ–‡ä»¶å¤¹
            kb_path = self.storage_path / kb_name
            if kb_path.exists():
                import shutil
                shutil.rmtree(kb_path)
            
            # ä»ç´¢å¼•ä¸­åˆ é™¤
            del self.knowledge_bases[kb_name]
            self._save_knowledge_bases_index()
            
            print(f"âœ… çŸ¥è¯†åº“ '{kb_name}' åˆ é™¤æˆåŠŸ")
            return True
        
        except Exception as e:
            logging.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            print(f"âŒ åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def update_knowledge_base(self, kb_name: str) -> bool:
        """æ›´æ–°çŸ¥è¯†åº“ï¼ˆé‡æ–°å¤„ç†æ–‡æ¡£ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if kb_name not in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
            return False
        
        kb_info = self.knowledge_bases[kb_name]
        
        print(f"ğŸ”„ æ›´æ–°çŸ¥è¯†åº“: {kb_name}")
        
        # åˆ é™¤æ—§æ•°æ®
        self.delete_knowledge_base(kb_name)
        
        # é‡æ–°åˆ›å»º
        return self.create_knowledge_base(
            kb_name, 
            kb_info.folder_path, 
            kb_info.description
        )