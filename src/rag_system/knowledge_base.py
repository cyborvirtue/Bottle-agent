#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGçŸ¥è¯†åº“ç®¡ç†æ¨¡å—
æ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€æ–‡æ¡£å¤„ç†ã€å‘é‡æ£€ç´¢
"""

import os
import json
import pickle
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import logging
from datetime import datetime

import faiss
import numpy as np
from dataclasses import dataclass, asdict

from .data_structures import DocumentChunk, Document
from .document_processor import DocumentProcessor
from .embedding_client import EmbeddingClient
from .agent_manager import AgentManager
from ..llm.llm_client import LLMClient


@dataclass
class KnowledgeBaseInfo:
    """çŸ¥è¯†åº“ä¿¡æ¯"""
    name: str
    description: str
    folder_path: str
    created_at: str
    updated_at: str
    document_count: int
    chunk_count: int


class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.rag_config = config["rag"]
        self.kb_config = config["knowledge_base"]
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.document_processor = DocumentProcessor(config)
        self.embedding_client = EmbeddingClient(config)
        self.llm_client = LLMClient(config)
        
        # å­˜å‚¨è·¯å¾„
        self.storage_path = Path(self.kb_config["storage_path"])
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–Agentç®¡ç†å™¨
        self.agent_manager = AgentManager(str(self.storage_path / "agents"))
        
        # åŠ è½½é¢„è®¾Agenté…ç½®
        try:
            presets_file = str(self.storage_path.parent / "examples" / "agent_presets.json")
            self.agent_manager.load_presets(presets_file)
        except Exception as e:
            logging.warning(f"âš ï¸ åŠ è½½é¢„è®¾Agenté…ç½®å¤±è´¥: {e}")
        
        # çŸ¥è¯†åº“ç´¢å¼•æ–‡ä»¶
        self.index_file = self.storage_path / "knowledge_bases.json"
        
        # åŠ è½½çŸ¥è¯†åº“ç´¢å¼•
        self.knowledge_bases = self._load_knowledge_bases_index()
    
    def _safe_kb_name(self, kb_name: str) -> str:
        """å°†çŸ¥è¯†åº“åç§°è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å¤¹åç§°
        
        Args:
            kb_name: åŸå§‹çŸ¥è¯†åº“åç§°
            
        Returns:
            å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°
        """
        # æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
        safe_name = kb_name.replace('/', '_').replace('\\', '_').replace(':', '_')
        safe_name = safe_name.replace('<', '_').replace('>', '_').replace('|', '_')
        safe_name = safe_name.replace('?', '_').replace('*', '_').replace('"', '_')
        return safe_name
    
    def _load_knowledge_bases_index(self) -> Dict[str, KnowledgeBaseInfo]:
        """åŠ è½½çŸ¥è¯†åº“ç´¢å¼•
        
        Returns:
            çŸ¥è¯†åº“ç´¢å¼•å­—å…¸
        """
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸ºKnowledgeBaseInfoå¯¹è±¡
                knowledge_bases = {}
                for name, info in data.items():
                    knowledge_bases[name] = KnowledgeBaseInfo(**info)
                
                return knowledge_bases
            except Exception as e:
                logging.error(f"åŠ è½½çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
        
        return {}
    
    def _save_knowledge_bases_index(self):
        """ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•"""
        try:
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            data = {}
            for name, info in self.knowledge_bases.items():
                data[name] = asdict(info)
            
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"ä¿å­˜çŸ¥è¯†åº“ç´¢å¼•å¤±è´¥: {e}")
    
    def create_knowledge_base(self, name: str, folder_path: str, description: str = "") -> bool:
        """åˆ›å»ºçŸ¥è¯†åº“
        
        Args:
            name: çŸ¥è¯†åº“åç§°
            folder_path: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
            description: çŸ¥è¯†åº“æè¿°
            
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        if name in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{name}' å·²å­˜åœ¨")
            return False
        
        folder_path = Path(folder_path)
        if not folder_path.exists():
            print(f"âŒ æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
            return False
        
        print(f"ğŸ“š åˆ›å»ºçŸ¥è¯†åº“: {name}")
        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {folder_path}")
        
        try:
            # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°ï¼‰
            safe_name = self._safe_kb_name(name)
            kb_path = self.storage_path / safe_name
            kb_path.mkdir(exist_ok=True)
            
            # å¤„ç†æ–‡æ¡£
            print("ğŸ“„ å¤„ç†æ–‡æ¡£...")
            documents = self.document_processor.process_folder(str(folder_path))
            
            if not documents:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
                return False
            
            # åˆ†å—
            print("âœ‚ï¸  æ–‡æ¡£åˆ†å—...")
            chunks = self.document_processor.chunk_documents(documents)
            
            # ç”ŸæˆåµŒå…¥
            print("ğŸ§® ç”ŸæˆåµŒå…¥å‘é‡...")
            embeddings = self.embedding_client.embed_texts([chunk.content for chunk in chunks])
            
            # æ·»åŠ åµŒå…¥åˆ°å—ä¸­
            for chunk, embedding in zip(chunks, embeddings):
                chunk.embedding = embedding
            
            # æ„å»ºå‘é‡ç´¢å¼•
            print("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
            self._build_vector_index(safe_name, chunks)
            
            # ä¿å­˜å—æ•°æ®
            self._save_chunks(safe_name, chunks)
            
            # æ›´æ–°çŸ¥è¯†åº“ä¿¡æ¯
            kb_info = KnowledgeBaseInfo(
                name=name,
                description=description,
                folder_path=str(folder_path),
                created_at=datetime.now().isoformat(),
                updated_at=datetime.now().isoformat(),
                document_count=len(documents),
                chunk_count=len(chunks)
            )
            
            self.knowledge_bases[name] = kb_info
            self._save_knowledge_bases_index()
            
            print(f"âœ… çŸ¥è¯†åº“ '{name}' åˆ›å»ºæˆåŠŸ")
            print(f"   ğŸ“Š æ–‡æ¡£æ•°: {len(documents)}, å—æ•°: {len(chunks)}")
            
            return True
        
        except Exception as e:
            logging.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            print(f"âŒ åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def _build_vector_index(self, kb_name: str, chunks: List[DocumentChunk]):
        """æ„å»ºå‘é‡ç´¢å¼•
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        if not chunks:
            return
        
        # è·å–åµŒå…¥ç»´åº¦
        embedding_dim = len(chunks[0].embedding)
        
        # åˆ›å»ºFAISSç´¢å¼•
        index = faiss.IndexFlatIP(embedding_dim)  # å†…ç§¯ç›¸ä¼¼åº¦
        
        # æ·»åŠ å‘é‡
        embeddings = np.array([chunk.embedding for chunk in chunks]).astype('float32')
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings)
        
        index.add(embeddings)
        
        # ä¿å­˜ç´¢å¼•
        index_path = self.storage_path / kb_name / "vector_index.faiss"
        faiss.write_index(index, str(index_path))
    
    def _save_chunks(self, kb_name: str, chunks: List[DocumentChunk]):
        """ä¿å­˜æ–‡æ¡£å—
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            chunks: æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks_path = self.storage_path / kb_name / "chunks.pkl"
        
        # ä¿å­˜æ—¶ä¸åŒ…å«åµŒå…¥å‘é‡ï¼ˆå¤ªå¤§ï¼‰
        chunks_to_save = []
        for chunk in chunks:
            chunk_copy = DocumentChunk(
                id=chunk.id,
                content=chunk.content,
                metadata=chunk.metadata
            )
            chunks_to_save.append(chunk_copy)
        
        with open(chunks_path, 'wb') as f:
            pickle.dump(chunks_to_save, f)
    
    def _load_chunks(self, kb_name: str) -> List[DocumentChunk]:
        """åŠ è½½æ–‡æ¡£å—
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks_path = self.storage_path / kb_name / "chunks.pkl"
        
        if not chunks_path.exists():
            return []
        
        with open(chunks_path, 'rb') as f:
            return pickle.load(f)
    
    def _load_vector_index(self, kb_name: str):
        """åŠ è½½å‘é‡ç´¢å¼•
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            FAISSç´¢å¼•
        """
        index_path = self.storage_path / kb_name / "vector_index.faiss"
        
        if not index_path.exists():
            return None
        
        return faiss.read_index(str(index_path))
    
    def query(self, kb_name: str, query: str, top_k: int = None) -> str:
        """æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›çš„ç›¸å…³å—æ•°é‡
            
        Returns:
            å›ç­”
        """
        if kb_name not in self.knowledge_bases:
            return f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨"
        
        if top_k is None:
            top_k = self.rag_config["top_k"]
        
        try:
            # åŠ è½½ç´¢å¼•å’Œå—
            index = self._load_vector_index(kb_name)
            chunks = self._load_chunks(kb_name)
            
            if index is None or not chunks:
                return f"âŒ çŸ¥è¯†åº“ '{kb_name}' æ•°æ®ä¸å®Œæ•´"
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_client.embed_text(query)
            query_vector = np.array([query_embedding]).astype('float32')
            faiss.normalize_L2(query_vector)
            
            # æœç´¢ç›¸ä¼¼å—
            scores, indices = index.search(query_vector, top_k)
            
            # è·å–ç›¸å…³å—
            relevant_chunks = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if score >= self.rag_config["similarity_threshold"]:
                    chunk = chunks[idx]
                    relevant_chunks.append((chunk, score))
            
            if not relevant_chunks:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk.content}\næ¥æº: {chunk.metadata.get('source', 'æœªçŸ¥')}"
                for i, (chunk, _) in enumerate(relevant_chunks)
            ])
            
            # ç”Ÿæˆå›ç­”
            prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€è¯¦ç»†ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µç¼–å·ã€‚

å›ç­”:
"""
            
            answer = self.llm_client.generate(prompt)
            
            # æ·»åŠ å¼•ç”¨ä¿¡æ¯
            references = "\n\nğŸ“š å‚è€ƒæ–‡æ¡£:\n"
            for i, (chunk, score) in enumerate(relevant_chunks):
                source = chunk.metadata.get('source', 'æœªçŸ¥')
                page = chunk.metadata.get('page', '')
                page_info = f", ç¬¬{page}é¡µ" if page else ""
                references += f"[{i+1}] {source}{page_info} (ç›¸ä¼¼åº¦: {score:.3f})\n"
            
            return answer + references
        
        except Exception as e:
            logging.error(f"æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {e}"
    
    def query_stream(self, kb_name: str, query: str, top_k: int = 5):
        """æµå¼æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°é‡
            
        Yields:
            ç”Ÿæˆçš„å›ç­”ç‰‡æ®µ
        """
        if kb_name not in self.knowledge_bases:
            yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨"
            return
        
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºçŸ¥è¯†åº“æŸ¥è¯¢è¯¦æƒ…
            print(f"ğŸ” [DEBUG] æŸ¥è¯¢çŸ¥è¯†åº“: '{kb_name}'")
            print(f"ğŸ” [DEBUG] å¯ç”¨çŸ¥è¯†åº“åˆ—è¡¨: {list(self.knowledge_bases.keys())}")
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨äºç´¢å¼•ä¸­: {kb_name in self.knowledge_bases}")
            
            # åŠ è½½çŸ¥è¯†åº“ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°ï¼‰
            safe_name = self._safe_kb_name(kb_name)
            print(f"ğŸ” [DEBUG] å®‰å…¨æ–‡ä»¶å¤¹åç§°: '{safe_name}'")
            
            kb_path = self.storage_path / safe_name
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“è·¯å¾„: {kb_path}")
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“è·¯å¾„æ˜¯å¦å­˜åœ¨: {kb_path.exists()}")
            
            # åŠ è½½å‘é‡ç´¢å¼•
            index_path = kb_path / "vector_index.faiss"  # ä¿®æ­£æ–‡ä»¶å
            print(f"ğŸ” [DEBUG] å‘é‡ç´¢å¼•è·¯å¾„: {index_path}")
            print(f"ğŸ” [DEBUG] å‘é‡ç´¢å¼•æ˜¯å¦å­˜åœ¨: {index_path.exists()}")
            
            if not index_path.exists():
                # å°è¯•æ—§çš„æ–‡ä»¶å
                old_index_path = kb_path / "index.faiss"
                print(f"ğŸ” [DEBUG] å°è¯•æ—§ç´¢å¼•è·¯å¾„: {old_index_path}")
                print(f"ğŸ” [DEBUG] æ—§ç´¢å¼•è·¯å¾„æ˜¯å¦å­˜åœ¨: {old_index_path.exists()}")
                
                if old_index_path.exists():
                    index_path = old_index_path
                else:
                    yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' ç´¢å¼•ä¸å­˜åœ¨\nè°ƒè¯•ä¿¡æ¯: å®‰å…¨åç§°='{safe_name}', è·¯å¾„={kb_path}"
                    return
            
            index = faiss.read_index(str(index_path))
            print(f"ğŸ” [DEBUG] æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•ï¼Œç»´åº¦: {index.d}, å‘é‡æ•°: {index.ntotal}")
            
            # åŠ è½½æ–‡æ¡£å—
            chunks_path = kb_path / "chunks.pkl"
            print(f"ğŸ” [DEBUG] æ–‡æ¡£å—è·¯å¾„: {chunks_path}")
            print(f"ğŸ” [DEBUG] æ–‡æ¡£å—æ˜¯å¦å­˜åœ¨: {chunks_path.exists()}")
            
            if not chunks_path.exists():
                yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' æ–‡æ¡£å—ä¸å­˜åœ¨\nè°ƒè¯•ä¿¡æ¯: è·¯å¾„={chunks_path}"
                return
            
            with open(chunks_path, 'rb') as f:
                chunks = pickle.load(f)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_client.embed_text(query)
            query_vector = np.array([query_embedding], dtype=np.float32)
            
            # æœç´¢ç›¸ä¼¼æ–‡æ¡£
            scores, indices = index.search(query_vector, top_k)
            
            # è·å–ç›¸å…³æ–‡æ¡£å—
            relevant_chunks = []
            for i, idx in enumerate(indices[0]):
                if idx < len(chunks):
                    relevant_chunks.append((chunks[idx], scores[0][i]))
            
            if not relevant_chunks:
                yield "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                return
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk.content}\næ¥æº: {chunk.metadata.get('source', 'æœªçŸ¥')}"
                for i, (chunk, _) in enumerate(relevant_chunks)
            ])
            
            # ç”Ÿæˆå›ç­”
            prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿å›ç­”å‡†ç¡®ã€è¯¦ç»†ï¼Œå¹¶å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚

ç”¨æˆ·é—®é¢˜: {query}

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µç¼–å·ã€‚

å›ç­”:
"""
            
            # æµå¼ç”Ÿæˆå›ç­”
            answer_parts = []
            for chunk in self.llm_client.generate_stream(prompt):
                answer_parts.append(chunk)
                yield chunk
            
            # æ·»åŠ å¼•ç”¨ä¿¡æ¯
            references = "\n\nğŸ“š å‚è€ƒæ–‡æ¡£:\n"
            for i, (chunk, score) in enumerate(relevant_chunks):
                source = chunk.metadata.get('source', 'æœªçŸ¥')
                page = chunk.metadata.get('page', '')
                page_info = f", ç¬¬{page}é¡µ" if page else ""
                references += f"[{i+1}] {source}{page_info} (ç›¸ä¼¼åº¦: {score:.3f})\n"
            
            yield references
        
        except Exception as e:
            logging.error(f"æµå¼æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            yield f"âŒ æŸ¥è¯¢å¤±è´¥: {e}"
    
    def query_stream_with_context(self, kb_name: str, query: str, chat_history: List[Dict[str, str]], top_k: int = 5, agent_name: str = "é»˜è®¤åŠ©æ‰‹"):
        """åŸºäºå¯¹è¯å†å²çš„æµå¼æŸ¥è¯¢çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            query: æŸ¥è¯¢é—®é¢˜
            chat_history: å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º[{"role": "user/assistant", "content": "..."}]
            top_k: è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°é‡
            
        Yields:
            ç”Ÿæˆçš„å›ç­”ç‰‡æ®µ
        """
        if kb_name not in self.knowledge_bases:
            yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨"
            return
        
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºçŸ¥è¯†åº“æŸ¥è¯¢è¯¦æƒ…
            print(f"ğŸ” [DEBUG] æŸ¥è¯¢çŸ¥è¯†åº“: '{kb_name}'")
            print(f"ğŸ” [DEBUG] å¯ç”¨çŸ¥è¯†åº“åˆ—è¡¨: {list(self.knowledge_bases.keys())}")
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨äºç´¢å¼•ä¸­: {kb_name in self.knowledge_bases}")
            
            # åŠ è½½çŸ¥è¯†åº“ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°ï¼‰
            safe_name = self._safe_kb_name(kb_name)
            print(f"ğŸ” [DEBUG] å®‰å…¨æ–‡ä»¶å¤¹åç§°: '{safe_name}'")
            
            kb_path = self.storage_path / safe_name
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“è·¯å¾„: {kb_path}")
            print(f"ğŸ” [DEBUG] çŸ¥è¯†åº“è·¯å¾„æ˜¯å¦å­˜åœ¨: {kb_path.exists()}")
            
            # åŠ è½½å‘é‡ç´¢å¼•
            index_path = kb_path / "vector_index.faiss"  # ä¿®æ­£æ–‡ä»¶å
            print(f"ğŸ” [DEBUG] å‘é‡ç´¢å¼•è·¯å¾„: {index_path}")
            print(f"ğŸ” [DEBUG] å‘é‡ç´¢å¼•æ˜¯å¦å­˜åœ¨: {index_path.exists()}")
            
            if not index_path.exists():
                # å°è¯•æ—§çš„æ–‡ä»¶å
                old_index_path = kb_path / "index.faiss"
                print(f"ğŸ” [DEBUG] å°è¯•æ—§ç´¢å¼•è·¯å¾„: {old_index_path}")
                print(f"ğŸ” [DEBUG] æ—§ç´¢å¼•è·¯å¾„æ˜¯å¦å­˜åœ¨: {old_index_path.exists()}")
                
                if old_index_path.exists():
                    index_path = old_index_path
                else:
                    yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' ç´¢å¼•ä¸å­˜åœ¨\nè°ƒè¯•ä¿¡æ¯: å®‰å…¨åç§°='{safe_name}', è·¯å¾„={kb_path}"
                    return
            
            index = faiss.read_index(str(index_path))
            print(f"ğŸ” [DEBUG] æˆåŠŸåŠ è½½å‘é‡ç´¢å¼•ï¼Œç»´åº¦: {index.d}, å‘é‡æ•°: {index.ntotal}")
            
            # åŠ è½½æ–‡æ¡£å—
            chunks_path = kb_path / "chunks.pkl"
            print(f"ğŸ” [DEBUG] æ–‡æ¡£å—è·¯å¾„: {chunks_path}")
            print(f"ğŸ” [DEBUG] æ–‡æ¡£å—æ˜¯å¦å­˜åœ¨: {chunks_path.exists()}")
            
            if not chunks_path.exists():
                yield f"âŒ çŸ¥è¯†åº“ '{kb_name}' æ–‡æ¡£å—ä¸å­˜åœ¨\nè°ƒè¯•ä¿¡æ¯: è·¯å¾„={chunks_path}"
                return
            
            with open(chunks_path, 'rb') as f:
                chunks = pickle.load(f)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_client.embed_text(query)
            query_vector = np.array([query_embedding], dtype=np.float32)
            
            # æœç´¢ç›¸ä¼¼æ–‡æ¡£
            scores, indices = index.search(query_vector, top_k)
            
            # è·å–ç›¸å…³æ–‡æ¡£å—
            relevant_chunks = []
            for i, idx in enumerate(indices[0]):
                if idx < len(chunks):
                    relevant_chunks.append((chunks[idx], scores[0][i]))
            
            if not relevant_chunks:
                yield "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
                return
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk.content}\næ¥æº: {chunk.metadata.get('source', 'æœªçŸ¥')}"
                for i, (chunk, _) in enumerate(relevant_chunks)
            ])
            
            # æ„å»ºåŒ…å«å¯¹è¯å†å²çš„æ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # ä½¿ç”¨æŒ‡å®šagentçš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.agent_manager.get_system_prompt(agent_name, context)
            if "å¯¹è¯å†å²" not in system_prompt:
                system_prompt += "\n\nå¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸ä¹‹å‰çš„å¯¹è¯ç›¸å…³ï¼Œè¯·ç»“åˆå¯¹è¯å†å²æ¥æä¾›è¿è´¯çš„å›ç­”ã€‚"
            
            messages.append({"role": "system", "content": system_prompt})
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆåªä¿ç•™æœ€è¿‘çš„å‡ è½®å¯¹è¯ä»¥é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿ï¼‰
            recent_history = chat_history[-6:] if len(chat_history) > 6 else chat_history
            for msg in recent_history:
                messages.append(msg)
            
            # æ·»åŠ å½“å‰é—®é¢˜
            messages.append({"role": "user", "content": query})
            
            # æµå¼ç”Ÿæˆå›ç­”
            answer_parts = []
            for chunk in self.llm_client.generate_stream_with_context(messages):
                answer_parts.append(chunk)
                yield chunk
            
            # æ·»åŠ å¼•ç”¨ä¿¡æ¯
            references = "\n\nğŸ“š å‚è€ƒæ–‡æ¡£:\n"
            for i, (chunk, score) in enumerate(relevant_chunks):
                source = chunk.metadata.get('source', 'æœªçŸ¥')
                page = chunk.metadata.get('page', '')
                page_info = f", ç¬¬{page}é¡µ" if page else ""
                references += f"[{i+1}] {source}{page_info} (ç›¸ä¼¼åº¦: {score:.3f})\n"
            
            yield references
        
        except Exception as e:
            logging.error(f"åŸºäºä¸Šä¸‹æ–‡çš„æµå¼æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            yield f"âŒ æŸ¥è¯¢å¤±è´¥: {e}"
    
    def list_knowledge_bases(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“
        
        Returns:
            çŸ¥è¯†åº“åç§°åˆ—è¡¨
        """
        return list(self.knowledge_bases.keys())
    
    def get_knowledge_base_info(self, kb_name: str) -> Optional[KnowledgeBaseInfo]:
        """è·å–çŸ¥è¯†åº“ä¿¡æ¯
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            çŸ¥è¯†åº“ä¿¡æ¯
        """
        return self.knowledge_bases.get(kb_name)
    
    def delete_knowledge_base(self, kb_name: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        if kb_name not in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
            return False
        
        try:
            # åˆ é™¤çŸ¥è¯†åº“æ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶å¤¹åç§°ï¼‰
            safe_name = self._safe_kb_name(kb_name)
            kb_path = self.storage_path / safe_name
            if kb_path.exists():
                import shutil
                shutil.rmtree(kb_path)
            
            # ä»ç´¢å¼•ä¸­åˆ é™¤
            del self.knowledge_bases[kb_name]
            self._save_knowledge_bases_index()
            
            print(f"âœ… çŸ¥è¯†åº“ '{kb_name}' åˆ é™¤æˆåŠŸ")
            return True
        
        except Exception as e:
            logging.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            print(f"âŒ åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def update_knowledge_base(self, kb_name: str) -> bool:
        """æ›´æ–°çŸ¥è¯†åº“ï¼ˆé‡æ–°å¤„ç†æ–‡æ¡£ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        if kb_name not in self.knowledge_bases:
            print(f"âŒ çŸ¥è¯†åº“ '{kb_name}' ä¸å­˜åœ¨")
            return False
        
        kb_info = self.knowledge_bases[kb_name]
        
        print(f"ğŸ”„ æ›´æ–°çŸ¥è¯†åº“: {kb_name}")
        
        # åˆ é™¤æ—§æ•°æ®
        self.delete_knowledge_base(kb_name)
        
        # é‡æ–°åˆ›å»º
        return self.create_knowledge_base(
            kb_name, 
            kb_info.folder_path, 
            kb_info.description
        )